from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
import asyncio
import logging
import re
from typing import List, Set, Optional
import os
import platform
from playwright_stealth import Stealth
import time
from free_proxy_manager import get_proxy_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmailExtractor:
    FAKE_EMAIL_PREFIXES = [
        "the", "2", "3", "4", "123", "20info", "aaa", "ab", "abc", "acc", 
        "acc_kaz", "account", "accounts", "accueil", "ad", "adi", "adm", 
        "an", "and", "available", "cc", "com", "domain", "domen", 
        "email", "fb", "foi", "for", "found", "get", "here", 
        "includes", "linkedin", "mailbox", "more", "my_name", "name", 
        "need", "nfo", "ninfo", "now", "online", "post", "sales2", 
        "test", "up", "we", "www", "xxx", "xxxxx", "username", 
        "firstname.lastname", "your.name", "unsubscribe"
    ]
    
    
    def __init__(self, headless: bool = False, use_proxy: bool = False):
        self.headless = headless
        self.use_proxy_fallback = use_proxy  # æ”¹åï¼šå¤±è´¥æ—¶æ‰ä½¿ç”¨ä»£ç†
        self.proxy_manager = get_proxy_manager(use_proxy=use_proxy) if use_proxy else None
        self.current_proxy = None
        self.playwright_instance = None
        self.browser = None
        self.context = None
        self.paused = False
        self.stopped = False
        self._pages = []  # è·Ÿè¸ªæ‰€æœ‰æ‰“å¼€çš„é¡µé¢
        self._failed_urls_needing_proxy = set()  # è®°å½•éœ€è¦ä»£ç†çš„URL
    
    def _extract_emails_from_text(self, text: str, domain: str = None) -> Set[str]:
        """Extract and filter emails from text"""
        if not text:
            return set()
        
        text = text.replace('\\n', ' ')
        pattern = r'\b[a-z\d\-][_a-z\d\-+]*(?:\.[_a-z\d\-+]*)*@[a-z\d]+[a-z\d\-]*(?:\.[a-z\d\-]+)*(?:\.[a-z]{2,63})\b'
        matches = re.findall(pattern, text, re.IGNORECASE)
        
        if not matches:
            return set()
        
        valid_emails = set()
        filtered_count = 0
        
        for email in matches:
            email = email.lower().strip()
            
            if email in valid_emails:
                continue
            
            if domain and domain not in email:
                logger.debug(f"è¿‡æ»¤é‚®ç®± (åŸŸåä¸åŒ¹é…): {email}")
                filtered_count += 1
                continue
            
            if email.endswith(('.png', '.jpg', '.gif', '.css', '.webp', '.crx1', '.js')):
                logger.debug(f"è¿‡æ»¤é‚®ç®± (æ–‡ä»¶åç¼€): {email}")
                filtered_count += 1
                continue
            
            original = email
            email = re.sub(r'^(x3|x2|u003|u0022|sx_mrsp_|3a)', '', email, flags=re.IGNORECASE)
            
            if email != original and not re.search(pattern, email, re.IGNORECASE):
                logger.debug(f"è¿‡æ»¤é‚®ç®± (æ¸…ç†åæ— æ•ˆ): {original}")
                filtered_count += 1
                continue
            
            if re.search(r'(no|not)[-|_]*reply|mailer[-|_]*daemon|reply.+\d{5,}', email, re.IGNORECASE):
                logger.debug(f"è¿‡æ»¤é‚®ç®± (spamæ¨¡å¼): {email}")
                filtered_count += 1
                continue
            
            if re.search(r'\d{13,}', email):
                logger.debug(f"è¿‡æ»¤é‚®ç®± (è¿‡å¤šæ•°å­—): {email}")
                filtered_count += 1
                continue
            
            spam_keywords = ['nondelivery', '@linkedin.com', '@sentry', '@linkedhelper.com', 'feedback', 'notification']
            if any(keyword in email for keyword in spam_keywords):
                logger.debug(f"è¿‡æ»¤é‚®ç®± (åƒåœ¾å…³é”®è¯): {email}")
                filtered_count += 1
                continue
            
            email_prefix = email.split('@')[0]
            if email_prefix in self.FAKE_EMAIL_PREFIXES:
                logger.info(f"è¿‡æ»¤é‚®ç®± (å‡å‰ç¼€): {email}")
                filtered_count += 1
                continue
            
            if email:
                logger.info(f"âœ“ æœ‰æ•ˆé‚®ç®±: {email}")
                valid_emails.add(email)
        
        if filtered_count > 0:
            logger.info(f"è¿‡æ»¤ {filtered_count} ä¸ª,ä¿ç•™ {len(valid_emails)} ä¸ªæœ‰æ•ˆé‚®ç®±")
        
        return valid_emails

    async def _create_context(self, use_proxy: bool = False):
        """åˆ›å»ºå¹¶é…ç½®ä¸€ä¸ªæ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        # è·å–ä»£ç†é…ç½®
        proxy_config = None
        if use_proxy and self.proxy_manager:
            proxy = self.proxy_manager.get_random_proxy()
            if proxy:
                proxy_config = proxy
                logger.info(f"âœ“ ä½¿ç”¨ä»£ç†: {proxy_config['server']}")
            else:
                logger.warning("âš  ä»£ç†ç®¡ç†å™¨æœªè¿”å›ä»£ç†ï¼Œä½¿ç”¨ç›´è¿")
        
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            locale="en-US",
            timezone_id="America/New_York",
            bypass_csp=True,
            ignore_https_errors=True,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            permissions=['geolocation'],
            proxy=proxy_config,
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br, zstd",
                "sec-ch-ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"Windows"',
                "sec-fetch-dest": "document",
                "sec-fetch-mode": "navigate",
                "sec-fetch-site": "none",
                "sec-fetch-user": "?1",
                "upgrade-insecure-requests": "1",
            },
        )

        # logger.info("åº”ç”¨ Stealth æ’ä»¶...")
        await Stealth().apply_stealth_async(context)

        # é¢å¤–çš„ JavaScript åæ£€æµ‹
        # logger.info("æ³¨å…¥é¢å¤–çš„åæ£€æµ‹è„šæœ¬...")
        await context.add_init_script("""
            // è¦†ç›– webdriver å±æ€§
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            
            // è¦†ç›– chrome å¯¹è±¡
            window.chrome = {
                runtime: {}
            };
            
            // è¦†ç›– permissions
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );
            
            // è¦†ç›– plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
            
            // è¦†ç›– languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en']
            });
        """)
        
        return context

    async def initialize(self, extension_path: str = None, use_proxy: bool = False):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ– Playwright...")
            self.playwright_instance = await async_playwright().start()
            
            args = [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-blink-features=AutomationControlled',
                '--no-first-run',
                '--no-zygote',
                '--disable-infobars',
                '--start-maximized',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process',
                '--disable-site-isolation-trials',
                '--disable-features=BlockInsecurePrivateNetworkRequests',
            ]

            logger.info(f"å¯åŠ¨æµè§ˆå™¨ (headless={self.headless})...")
            self.browser = await self.playwright_instance.chromium.launch(
                headless=self.headless,
                args=args,
            )

            logger.info("åˆ›å»ºä¸»æµè§ˆå™¨ä¸Šä¸‹æ–‡...")
            self.context = await self._create_context(use_proxy=use_proxy)

            # éªŒè¯å¯åŠ¨
            test_page = await self.context.new_page()
            await test_page.close()
            
            logger.info(f"æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ (headless={self.headless})")
            return self
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            await self.close()
            raise

            # éªŒè¯å¯åŠ¨
            test_page = await self.context.new_page()
            await test_page.close()
            
            logger.info(f"æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ (headless={self.headless})")
            return self
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            await self.close()
            raise
           # é”™è¯¯åˆ†ç±»è¾…åŠ©æ–¹æ³•
    def _categorize_error(self, error_str: str) -> tuple:
        """åˆ†ç±»é”™è¯¯ç±»å‹å¹¶è¿”å› (é”™è¯¯ç±»å‹, æ˜¯å¦å¯é‡è¯•, å»ºè®®å»¶è¿Ÿç§’æ•°)"""
        error_lower = error_str.lower()
        
        # ç½‘ç»œè¿æ¥é”™è¯¯ - å¯é‡è¯•
        if any(keyword in error_lower for keyword in [
            'err_socket_not_connected', 'err_connection_refused', 
            'err_connection_reset', 'err_connection_closed',
            'connection refused', 'socket', 'network'
        ]):
            return ('NETWORK_ERROR', True, 3)
        
        # è¶…æ—¶é”™è¯¯ - å¯é‡è¯•
        if any(keyword in error_lower for keyword in [
            'timeout', 'timed out', 'err_timed_out'
        ]):
            return ('TIMEOUT_ERROR', True, 2)
        
        # DNSé”™è¯¯ - å¯é‡è¯•
        if any(keyword in error_lower for keyword in [
            'dns', 'err_name_not_resolved', 'getaddrinfo failed'
        ]):
            return ('DNS_ERROR', True, 5)
        
        # CAPTCHA/åçˆ¬è™« - éœ€è¦ä»£ç†
        if any(keyword in error_lower for keyword in [
            'captcha', 'robot', 'challenge', 'cloudflare'
        ]):
            return ('ANTI_SCRAPING', True, 1)
        
        # æœåŠ¡å™¨é”™è¯¯ - å¯é‡è¯•
        if any(keyword in error_lower for keyword in [
            '500', '502', '503', '504', 'server error'
        ]):
            return ('SERVER_ERROR', True, 5)
        
        # å®¢æˆ·ç«¯é”™è¯¯ - ä¸å¯é‡è¯•
        if any(keyword in error_lower for keyword in [
            '400', '401', '403', '404', '405'
        ]):
            return ('CLIENT_ERROR', False, 0)
        
        # æœªçŸ¥é”™è¯¯ - è°¨æ…é‡è¯•
        return ('UNKNOWN_ERROR', True, 2)
    
    # æŸ¥æ‰¾è‹±æ–‡é“¾æ¥
    async def _find_english_link(self, page) -> str:
        """æŸ¥æ‰¾è‹±æ–‡é“¾æ¥"""
        try:
            english_url = await page.evaluate("""() => {
                const links = Array.from(document.querySelectorAll('a'));
                for (const link of links) {
                    const text = link.innerText.trim().toLowerCase();
                    const href = link.href.toLowerCase();
                    const title = (link.title || '').toLowerCase();
                    const ariaLabel = (link.getAttribute('aria-label') || '').toLowerCase();
                    
                    if (text === 'english' || text === 'en' || text.includes('english version')) {
                        return link.href;
                    }
                    
                    if (title.includes('english') || ariaLabel.includes('english')) {
                        return link.href;
                    }
                    
                    if ((href.includes('/en/') || href.endsWith('/en')) && !window.location.href.includes('/en/')) {
                        return link.href;
                    }
                }
                return null;
            }""")
            
            return english_url
        except Exception as e:
            logger.warning(f"æŸ¥æ‰¾è‹±æ–‡é“¾æ¥å‡ºé”™: {str(e)}")
            return None

    async def _extract_from_page(self, page, retry_if_empty: bool = True) -> Set[str]:
        """ä»å½“å‰é¡µé¢æå–é‚®ç®±"""
        try:
            ready_state = await page.evaluate('document.readyState')
            logger.debug(f"é¡µé¢çŠ¶æ€: {ready_state}")
            
            page_html = await page.content()
            page_text = await page.inner_text('body')
            
            logger.debug(f"HTMLé•¿åº¦: {len(page_html)}, æ–‡æœ¬é•¿åº¦: {len(page_text)}")
            
            emails_from_html = self._extract_emails_from_text(page_html)
            emails_from_text = self._extract_emails_from_text(page_text)
            
            all_emails = emails_from_html.union(emails_from_text)
            
            # å¦‚æœç¬¬ä¸€æ¬¡æ²¡æ‰¾åˆ°é‚®ç®±ï¼Œç­‰å¾…ä¸€ä¸‹å†è¯•ä¸€æ¬¡ï¼ˆå¯èƒ½æ˜¯åŠ¨æ€åŠ è½½ï¼‰
            if len(all_emails) == 0 and retry_if_empty:
                logger.debug("é¦–æ¬¡æœªæ‰¾åˆ°é‚®ç®±ï¼Œç­‰å¾…2ç§’åé‡è¯•...")
                await asyncio.sleep(2)
                
                # é‡æ–°è·å–å†…å®¹
                page_html = await page.content()
                page_text = await page.inner_text('body')
                
                emails_from_html = self._extract_emails_from_text(page_html)
                emails_from_text = self._extract_emails_from_text(page_text)
                
                all_emails = emails_from_html.union(emails_from_text)
                if len(all_emails) > 0:
                    logger.info(f"é‡è¯•åæ‰¾åˆ° {len(all_emails)} ä¸ªé‚®ç®±")
            
            logger.info(f"æœ¬æ¬¡æå–æ‰¾åˆ° {len(all_emails)} ä¸ªé‚®ç®±")
            
            return all_emails
        except Exception as e:
            logger.error(f"é¡µé¢æå–å¤±è´¥: {str(e)}", exc_info=True)
            return set()
    
    # ä»å•ä¸ªURLæå–é‚®ç®±,è¿”å›è¯¦ç»†ç»“æœ
    async def extract_from_url(self, url: str, callback=None, max_attempts: int = 3, context=None) -> dict:
        """ä»å•ä¸ªURLæå–é‚®ç®±ï¼Œè¿”å›è¯¦ç»†ç»“æœ"""
        emails = set()
        visited_urls = set()
        page = None
        error_message = None
        error_type = None
        success = False
        last_error = None
        
        # ä½¿ç”¨ä¼ å…¥çš„ context æˆ–é»˜è®¤ context
        current_context = context or self.context
        
        # è·å–é‡è¯•è¶…æ—¶å€æ•°
        retry_timeout_multiplier = float(os.getenv("RETRY_TIMEOUT_MULTIPLIER", "1.5"))

        for attempt in range(max_attempts):
            try:
                # æ£€æŸ¥æ˜¯å¦åœæ­¢
                if self.stopped:
                    logger.info("æ£€æµ‹åˆ°åœæ­¢ä¿¡å·,ç»ˆæ­¢æå–")
                    return {
                        'url': url,
                        'emails': list(emails),
                        'count': len(emails),
                        'success': False,
                        'error': 'ç”¨æˆ·åœæ­¢',
                        'error_type': 'STOPPED',
                        'attempts': attempt + 1
                    }
                
                # æ£€æŸ¥æµè§ˆå™¨ä¸Šä¸‹æ–‡æ˜¯å¦æœ‰æ•ˆ
                if not current_context:
                    logger.error("æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å­˜åœ¨,æ— æ³•ç»§ç»­")
                    return {
                        'url': url,
                        'emails': list(emails),
                        'count': 0,
                        'success': False,
                        'error': 'æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸å­˜åœ¨',
                        'error_type': 'BROWSER_CONTEXT_MISSING',
                        'attempts': attempt + 1
                    }
                
                # æ˜¾ç¤ºå½“å‰å°è¯•æ¬¡æ•°
                attempt_msg = f"ç¬¬ {attempt + 1}/{max_attempts} æ¬¡å°è¯•"
                if attempt > 0:
                    attempt_msg = f"é‡è¯•ä¸­ ({attempt_msg})"
                logger.info(f"æ­£åœ¨è®¿é—®: {url} ({attempt_msg})")
                
                page = await current_context.new_page()
                self._pages.append(page)
                
                # è·å–è¶…æ—¶è®¾ç½®,é»˜è®¤ä¸º 60000ms (60ç§’)
                # åœ¨ Render ç­‰æ…¢é€Ÿç¯å¢ƒä¸­,è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´å¯ä»¥å‡å°‘å› ç½‘ç»œæ³¢åŠ¨å¯¼è‡´çš„å¤±è´¥
                base_timeout = int(os.getenv("PAGE_TIMEOUT", "60000"))
                # é‡è¯•æ—¶å¢åŠ è¶…æ—¶æ—¶é—´
                page_timeout = int(base_timeout * (retry_timeout_multiplier ** attempt))
                logger.info(f"è®¾ç½®é¡µé¢è¶…æ—¶: {page_timeout}ms (å°è¯• {attempt + 1}/{max_attempts})")
                page.set_default_timeout(page_timeout)

                # æ·»åŠ éšæœºå»¶è¿Ÿ
                await asyncio.sleep(0.5 + (hash(url) % 10) / 10)

                # è®¿é—®é¡µé¢ - ä½¿ç”¨æ›´å®½æ¾çš„ç­‰å¾…ç­–ç•¥
                # ç§»é™¤ asyncio.wait_for,ç›´æ¥ä½¿ç”¨ Playwright çš„ timeout,é¿å… Future exception was never retrieved é”™è¯¯
                await page.goto(url, wait_until='domcontentloaded', timeout=page_timeout)
                
                visited_urls.add(url)
                
                # ç­‰å¾…ç½‘ç»œç©ºé—² - ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ
                try:
                    logger.debug(f"ç­‰å¾…ç½‘ç»œç©ºé—²...")
                    await page.wait_for_load_state('networkidle', timeout=10000)
                    logger.debug(f"ç½‘ç»œå·²ç©ºé—²")
                except Exception as e:
                    logger.debug(f"ç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶(è¿™æ˜¯æ­£å¸¸çš„): {str(e)}")
                
                # å¢åŠ ç­‰å¾…æ—¶é—´,è®© JavaScript æœ‰è¶³å¤Ÿæ—¶é—´æ¸²æŸ“å†…å®¹
                # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­,èµ„æºå—é™å¯èƒ½å¯¼è‡´ JS æ‰§è¡Œè¾ƒæ…¢,ä½† 3ç§’å¯èƒ½å¤ªé•¿
                await asyncio.sleep(1)


                if callback:
                    await callback('log', f"ğŸ“„ é¡µé¢åŠ è½½å®Œæˆ: {url}", 'success')
                
                # è®°å½•é¡µé¢ä¿¡æ¯ç”¨äºè°ƒè¯•å¹¶æ£€æµ‹éªŒè¯ç 
                try:
                    page_title = await page.title()
                    page_url = page.url
                    logger.info(f"é¡µé¢æ ‡é¢˜: {page_title}")
                    logger.info(f"æœ€ç»ˆURL: {page_url}")
                    
                    # æ£€æµ‹æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯ç /æœºå™¨äººæ£€æµ‹é¡µé¢
                    captcha_indicators = [
                        'captcha', 'robot', 'challenge', 'verification',
                        'security check', 'are you human', 'prove you',
                        'sgcaptcha', 'cloudflare', 'recaptcha'
                    ]
                    
                    page_title_lower = page_title.lower()
                    page_url_lower = page_url.lower()
                    
                    is_captcha = any(
                        indicator in page_title_lower or indicator in page_url_lower
                        for indicator in captcha_indicators
                    )
                    
                    if is_captcha:
                        error_message = f"ç½‘ç«™å¯ç”¨äº†åçˆ¬è™«éªŒè¯ (CAPTCHA/Robot Challenge)"
                        error_type = 'ANTI_SCRAPING'
                        logger.warning(f"âŒ {url} - {error_message}")
                        logger.warning(f"   æ£€æµ‹åˆ°: æ ‡é¢˜='{page_title}', URLåŒ…å«éªŒè¯ç è·¯å¾„")
                        
                        # å¦‚æœå¯ç”¨äº†ä»£ç†å›é€€ä¸”è¿™æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œè§¦å‘é‡è¯•
                        if self.use_proxy_fallback and attempt == 0:
                            logger.info(f"ğŸ”„ å°†ä½¿ç”¨ä»£ç†é‡è¯•: {url}")
                            if callback:
                                await callback('log', f"ğŸ”„ æ£€æµ‹åˆ°CAPTCHAï¼Œå°†ä½¿ç”¨ä»£ç†é‡è¯•...", 'warning')
                            # æŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
                            raise Exception("CAPTCHA_DETECTED_RETRY_WITH_PROXY")
                        else:
                            # å·²ç»ç”¨è¿‡ä»£ç†æˆ–æœªå¯ç”¨ä»£ç†å›é€€ï¼Œç›´æ¥å¤±è´¥
                            if callback:
                                await callback('log', f"âš ï¸ {url} - è¢«åçˆ¬è™«ç³»ç»Ÿæ‹¦æˆª", 'warning')
                            
                            return {
                                'url': url,
                                'emails': [],
                                'count': 0,
                                'success': False,
                                'error': error_message,
                                'error_type': error_type,
                                'attempts': attempt + 1
                            }
                    
                except Exception as e:
                    logger.debug(f"è·å–é¡µé¢ä¿¡æ¯å¤±è´¥: {e}")

                # æå–é‚®ç®±
                current_emails = await self._extract_from_page(page)
                emails.update(current_emails)

                if current_emails and callback:
                    await callback('log', f"ğŸ“§ ä»å½“å‰é¡µé¢æå–åˆ° {len(current_emails)} ä¸ªé‚®ç®±", 'success')
                    await callback('email', list(emails))

                # å°è¯•è‹±æ–‡ç‰ˆ
                if not self.stopped:
                    english_url = await self._find_english_link(page)
                    if english_url and english_url not in visited_urls and '/en/' not in url:
                        if callback:
                            await callback('log', f"ğŸŒ å‘ç°è‹±æ–‡ç‰ˆé¡µé¢,æ­£åœ¨è·³è½¬...", 'info')
                        try:
                            # è‹±æ–‡ç‰ˆé¡µé¢è·³è½¬è¶…æ—¶è®¾ä¸ºä¸»è¦è¶…æ—¶çš„ä¸€åŠï¼Œä½†è‡³å°‘ 10ç§’
                            # æ³¨æ„ï¼šè¿™é‡Œé‡æ–°è·å– page_timeout æ˜¯ä¸ºäº†å®‰å…¨ï¼Œè™½ç„¶ä¸Šé¢å·²ç»è·å–è¿‡äº†ï¼Œä½†ä¸ºäº†ä¿æŒå±€éƒ¨å˜é‡æ¸…æ™°
                            page_timeout = int(os.getenv("PAGE_TIMEOUT", "60000"))
                            english_timeout = max(10000, page_timeout // 2)
                            await page.goto(english_url, wait_until='domcontentloaded', timeout=english_timeout)
                            visited_urls.add(english_url)
                            await asyncio.sleep(2)
                            
                            english_page_emails = await self._extract_from_page(page)
                            new_emails = english_page_emails - emails
                            if new_emails:
                                emails.update(new_emails)
                                if callback:
                                    await callback('log', f"ğŸ“§ ä»è‹±æ–‡ç‰ˆé¢å¤–æå–åˆ° {len(new_emails)} ä¸ªé‚®ç®±", 'success')
                                    await callback('email', list(emails))
                        except Exception as e:
                            logger.warning(f"è®¿é—®è‹±æ–‡ç‰ˆå¤±è´¥: {str(e)}")

                # æˆåŠŸ
                success = True
                if attempt > 0 and callback:
                    await callback('log', f"âœ… é‡è¯•æˆåŠŸ (ç¬¬ {attempt + 1} æ¬¡å°è¯•)", 'success')
                # No break here, the finally block will handle the return on success
            except PlaywrightTimeout as e:
                error_message = f"é¡µé¢åŠ è½½è¶…æ—¶: {str(e)}"
                last_error = error_message
                error_type, should_retry, retry_delay = self._categorize_error(error_message)
                
                logger.warning(f"â±ï¸ [{error_type}] {url} - {error_message}")
                logger.info(f"   é”™è¯¯åˆ†ç±»: {error_type}, å¯é‡è¯•: {should_retry}, å»ºè®®å»¶è¿Ÿ: {retry_delay}ç§’")
                
                if callback:
                    await callback('log', f"â±ï¸ {url[:50]}... - è¶…æ—¶ (å°è¯• {attempt + 1}/{max_attempts})", 'warning')
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šä¸”é”™è¯¯å¯é‡è¯•
                if attempt < max_attempts - 1 and should_retry:
                    logger.info(f"ğŸ”„ å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
                    if callback:
                        await callback('log', f"ğŸ”„ ç­‰å¾… {retry_delay}ç§’åé‡è¯•...", 'info')
                    await asyncio.sleep(retry_delay)
                
            except Exception as e:
                error_message = str(e)
                last_error = error_message
                error_type, should_retry, retry_delay = self._categorize_error(error_message)
                
                logger.error(f"âŒ [{error_type}] {url} - {error_message}")
                logger.info(f"   é”™è¯¯è¯¦æƒ…: ç±»å‹={error_type}, å¯é‡è¯•={should_retry}, å»ºè®®å»¶è¿Ÿ={retry_delay}ç§’")
                logger.info(f"   å½“å‰å°è¯•: {attempt + 1}/{max_attempts}")
                
                if callback:
                    await callback('log', f"âŒ é”™è¯¯: {url[:50]}... - {error_type} (å°è¯• {attempt + 1}/{max_attempts})", 'error')
                
                # å¦‚æœæ˜¯ CAPTCHA è§¦å‘çš„ä»£ç†é‡è¯•
                if "CAPTCHA_DETECTED_RETRY_WITH_PROXY" in error_message and attempt == 0:
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°CAPTCHAï¼Œå‡†å¤‡ä½¿ç”¨ä»£ç†é‡è¯•...")
                    
                    # åˆ›å»ºä¸´æ—¶ä»£ç†ä¸Šä¸‹æ–‡
                    proxy_context = None
                    try:
                        proxy_context = await self._create_context(use_proxy=True)
                        logger.info(f"âœ“ å·²åˆ›å»ºä¸´æ—¶ä»£ç†ä¸Šä¸‹æ–‡ï¼Œé‡æ–°å°è¯•...")
                        if callback:
                            await callback('log', f"âœ“ å·²åˆ‡æ¢åˆ°ä»£ç†æ¨¡å¼ï¼Œé‡æ–°å°è¯•...", 'info')
                        
                        # é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨æ–°çš„ä¸Šä¸‹æ–‡
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªé‡è¯•ä¸€æ¬¡ (max_attempts=1)ï¼Œæˆ–è€…æ ¹æ®éœ€è¦è°ƒæ•´
                        retry_result = await self.extract_from_url(url, callback, max_attempts=max_attempts, context=proxy_context)
                        return retry_result
                        
                    except Exception as retry_error:
                        logger.error(f"ä½¿ç”¨ä»£ç†é‡è¯•å¤±è´¥: {retry_error}")
                        error_message = f"ä»£ç†é‡è¯•å¤±è´¥: {str(retry_error)}"
                        error_type = 'PROXY_RETRY_FAILED'
                        break # ä»£ç†é‡è¯•å¤±è´¥ï¼Œç›´æ¥è·³å‡º
                    finally:
                        # ç¡®ä¿å…³é—­ä¸´æ—¶ä¸Šä¸‹æ–‡
                        if proxy_context:
                            try:
                                await proxy_context.close()
                            except:
                                pass
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šä¸”é”™è¯¯å¯é‡è¯•
                if attempt < max_attempts - 1 and should_retry:
                    logger.info(f"ğŸ”„ å°†åœ¨ {retry_delay} ç§’åé‡è¯•...")
                    if callback:
                        await callback('log', f"ğŸ”„ [{error_type}] ç­‰å¾… {retry_delay}ç§’åé‡è¯•...", 'info')
                    await asyncio.sleep(retry_delay)
                elif not should_retry:
                    logger.warning(f"âš ï¸ é”™è¯¯ç±»å‹ {error_type} ä¸å»ºè®®é‡è¯•ï¼Œè·³è¿‡å‰©ä½™å°è¯•")
                    if callback:
                        await callback('log', f"âš ï¸ {error_type} - ä¸å¯é‡è¯•ï¼Œè·³è¿‡", 'warning')
                    break
            
            finally:
                # å…³é—­é¡µé¢
                if page:
                    try:
                        await page.close()
                        if page in self._pages:
                            self._pages.remove(page)
                    except:
                        pass
                
                # å¦‚æœæˆåŠŸæå–åˆ°é‚®ç®±ï¼Œç«‹å³è¿”å›
                if success:
                    logger.info(f"âœ… æˆåŠŸä» {url} æå–åˆ° {len(emails)} ä¸ªé‚®ç®± (å°è¯• {attempt + 1}/{max_attempts})")
                    return {
                        'url': url,
                        'emails': list(emails),
                        'count': len(emails),
                        'success': True,
                        'error': None,
                        'error_type': None,
                        'attempts': attempt + 1
                    }
    
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
        final_error = last_error or error_message or 'æœªçŸ¥é”™è¯¯'
        logger.warning(f"âŒ [{error_type or 'UNKNOWN'}] {url} - æ‰€æœ‰ {max_attempts} æ¬¡å°è¯•å‡å¤±è´¥")
        logger.warning(f"   æœ€ç»ˆé”™è¯¯: {final_error}")
        
        if callback:
            await callback('log', f"âŒ {url[:50]}... - å¤±è´¥ [{error_type or 'UNKNOWN'}]: {final_error[:50]}", 'error')
        
        return {
            'url': url,
            'emails': list(emails),
            'count': len(emails),
            'success': False,
            'error': final_error,
            'error_type': error_type or 'UNKNOWN',
            'attempts': max_attempts
        }
    
    async def extract_from_urls(self, urls: List[str], callback=None) -> dict:
        """æ‰¹é‡æå–é‚®ç®±ï¼Œè¿”å›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ (å¹¶è¡Œç‰ˆ)"""
        all_emails = set()
        total = len(urls)
        failed_urls = []
        no_email_urls = []
        
        # çº¿ç¨‹å®‰å…¨çš„é”
        results_lock = asyncio.Lock()
        
        logger.info(f"å¼€å§‹æ‰¹é‡æå– {total} ä¸ªURL (å¹¶è¡Œ)")
        
        start_time = time.time()
        
        # é™åˆ¶å¹¶å‘æ•° - ä»ç¯å¢ƒå˜é‡è·å–ï¼Œé»˜è®¤ä¸º 3 (é€‚åˆ Render ç­‰å®¹å™¨ç¯å¢ƒ)
        max_concurrency = int(os.getenv("MAX_CONCURRENCY", "3"))
        logger.info(f"å¹¶å‘é™åˆ¶: {max_concurrency}")
        sem = asyncio.Semaphore(max_concurrency)
        
        # è¿›åº¦è®¡æ•°å™¨
        completed_count = 0
        progress_lock = asyncio.Lock()
        
        async def process_url(index, url):
            nonlocal completed_count
            
            async with sem:
                # æ£€æŸ¥æš‚åœ/åœæ­¢
                while self.paused and not self.stopped:
                    await asyncio.sleep(0.5)
                
                if self.stopped:
                    return
                
                logger.info(f"ğŸ“Š å¼€å§‹å¤„ç†: {url}")
                if callback:
                    await callback('log', f"ğŸ” æ­£åœ¨å¤„ç†: {url[:50]}...", 'info')
                
                try:
                    result = await self.extract_from_url(url, callback)
                    
                    async with results_lock:
                        # æ›´æ–°æ€»é‚®ç®±åˆ—è¡¨
                        all_emails.update(result['emails'])
                        
                        # è·Ÿè¸ªå¤±è´¥å’Œæ— é‚®ç®±çš„URL
                        if not result['success']:
                            failed_urls.append({
                                'url': url,
                                'error': result['error'] or 'æœªçŸ¥é”™è¯¯',
                                'timestamp': time.time()
                            })
                        elif result['count'] == 0:
                            no_email_urls.append({
                                'url': url,
                                'timestamp': time.time()
                            })
                except Exception as e:
                    logger.error(f"å¤„ç† {url} æ—¶å‡ºé”™: {e}")
                    async with results_lock:
                        failed_urls.append({
                            'url': url,
                            'error': str(e),
                            'timestamp': time.time()
                        })
                    if callback:
                        await callback('log', f"âŒ è·³è¿‡ {url}: {str(e)}", 'error')
                finally:
                    # æ›´æ–°è¿›åº¦
                    async with progress_lock:
                        completed_count += 1
                        current_progress = int(completed_count / total * 100)
                    
                    if callback:
                        await callback('progress', current_progress)
                
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [process_url(i, url) for i, url in enumerate(urls)]
        
        # è¿è¡Œæ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*tasks)
        
        # å‘é€ç»Ÿè®¡ä¿¡æ¯
        if callback:
            await callback('failed_urls', failed_urls)
            await callback('no_email_urls', no_email_urls)
        
        end_time = time.time()
        duration = end_time - start_time
        duration_str = f"{duration:.2f}ç§’"
        
        if callback and not self.stopped:
            await callback('log', f"âœ… æå–å®Œæˆ!å…± {len(all_emails)} ä¸ªå”¯ä¸€é‚®ç®±", 'success')
            await callback('log', f"ğŸ“Š ç»Ÿè®¡: æˆåŠŸ {total - len(failed_urls)} ä¸ª, å¤±è´¥ {len(failed_urls)} ä¸ª, æ— é‚®ç®± {len(no_email_urls)} ä¸ª. æ€»è€—æ—¶: {duration_str}", 'info')

        logger.info(f"æ‰¹é‡æå–å®Œæˆ: {len(all_emails)} ä¸ªé‚®ç®±, {len(failed_urls)} ä¸ªå¤±è´¥, {len(no_email_urls)} ä¸ªæ— é‚®ç®±, è€—æ—¶: {duration_str}")
        
        return {
            'emails': list(all_emails),
            'failed_urls': failed_urls,
            'no_email_urls': no_email_urls,
            'total_processed': total,
            'total_emails': len(all_emails),
            'duration': duration
        }
    
    def pause(self):
        """æš‚åœæå–"""
        self.paused = True
        logger.info("æå–å·²æš‚åœ")
    
    def resume(self):
        """ç»§ç»­æå–"""
        self.paused = False
        logger.info("æå–å·²ç»§ç»­")
    
    def stop(self):
        """åœæ­¢æå–"""
        self.stopped = True
        self.paused = False
        logger.info("æå–å·²åœæ­¢")
    
    async def close(self):
        """å½»åº•å…³é—­æµè§ˆå™¨"""
        logger.info("å¼€å§‹å…³é—­æµè§ˆå™¨èµ„æº...")
        
        # è®¾ç½®åœæ­¢æ ‡å¿—,é˜²æ­¢æ–°æ“ä½œ
        self.stopped = True
        
        try:
            # 1. å…³é—­æ‰€æœ‰æ‰“å¼€çš„é¡µé¢
            if self._pages:
                logger.info(f"å…³é—­ {len(self._pages)} ä¸ªæ‰“å¼€çš„é¡µé¢...")
                pages_to_close = self._pages[:]  # åˆ›å»ºå‰¯æœ¬
                for page in pages_to_close:
                    try:
                        if not page.is_closed():
                            await asyncio.wait_for(page.close(), timeout=5.0)
                            logger.debug(f"é¡µé¢å·²å…³é—­")
                    except asyncio.TimeoutError:
                        logger.warning(f"å…³é—­é¡µé¢è¶…æ—¶")
                    except Exception as e:
                        logger.warning(f"å…³é—­é¡µé¢å‡ºé”™: {e}")
                self._pages.clear()
                logger.info("æ‰€æœ‰é¡µé¢å·²å…³é—­")
            
            # 2. å…³é—­ä¸Šä¸‹æ–‡
            if self.context:
                try:
                    await asyncio.wait_for(self.context.close(), timeout=10.0)
                    logger.info("BrowserContext å·²å…³é—­")
                except asyncio.TimeoutError:
                    logger.warning("å…³é—­ context è¶…æ—¶")
                except Exception as e:
                    logger.warning(f"å…³é—­ context æ—¶å‡ºé”™: {e}")
                finally:
                    self.context = None
            
            # 3. å…³é—­æµè§ˆå™¨
            if self.browser:
                try:
                    await asyncio.wait_for(self.browser.close(), timeout=10.0)
                    logger.info("Browser å·²å…³é—­")
                except asyncio.TimeoutError:
                    logger.warning("å…³é—­ browser è¶…æ—¶")
                except Exception as e:
                    logger.warning(f"å…³é—­ browser æ—¶å‡ºé”™: {e}")
                finally:
                    self.browser = None
            
            # 4. åœæ­¢ Playwright
            if self.playwright_instance:
                try:
                    await asyncio.wait_for(self.playwright_instance.stop(), timeout=10.0)
                    logger.info("Playwright å·²åœæ­¢")
                except asyncio.TimeoutError:
                    logger.warning("åœæ­¢ playwright è¶…æ—¶")
                except Exception as e:
                    logger.warning(f"åœæ­¢ playwright æ—¶å‡ºé”™: {e}")
                finally:
                    self.playwright_instance = None
            
            # 5. é‡ç½®çŠ¶æ€
            self.stopped = False
            self.paused = False
            
            # 6. ç­‰å¾…èµ„æºå®Œå…¨é‡Šæ”¾
            await asyncio.sleep(1.0)  # å¢åŠ åˆ°1ç§’ç¡®ä¿å®Œå…¨é‡Šæ”¾
            
            logger.info("æµè§ˆå™¨èµ„æºå·²å®Œå…¨é‡Šæ”¾")
            
        except Exception as e:
            logger.error(f"å…³é—­æµè§ˆå™¨æ—¶å‡ºé”™: {e}", exc_info=True)
        
